import pandas as pd
import sqlite3
import numpy as np
from pathlib import Path

import yaml
BASE_DIR = Path(__file__).resolve().parent
CONFIG_PATH = BASE_DIR / "config" / "clean_config.yaml"

import warnings
import pandas as _pd
# å…¼å®¹æœªæ¥ pandas è¡Œä¸ºï¼Œé¿ï¿½?bfill/ffill ä¸‹çš„ downcasting è­¦å‘Š
_pd.set_option('future.no_silent_downcasting', True)

def read_excel_file(path):
    """æ™ºèƒ½è¯»å–å•ä¸ª Excel/HTML æ–‡ä»¶ï¿½?
    - æ ¹æ®æ‰©å±•åä¼˜å…ˆé€‰æ‹© engine
    - ï¿½?.xls æ— æ³•è¯»å–æ—¶å°ï¿½?pd.read_html(..., encoding='gbk')ï¼ˆCNKIå¸¸è§ï¿½?
    - è¿”å› DataFrame æˆ–æŠ›å‡ºå¼‚ï¿½?
    """
    p = Path(path)
    ext = p.suffix.lower()
    # å°è¯•æŒ‰æ‰©å±•åé€‰æ‹© engine
    try:
        if ext in ('.xlsx', '.xlsm', '.xltx', '.xltm'):
            return pd.read_excel(path, engine='openpyxl')
        if ext == '.xlsb':
            return pd.read_excel(path, engine='pyxlsb')
        if ext == '.xls':
            # ï¿½?.xls æ–‡ä»¶ï¼Œå…ˆå°è¯• xlrd
            try:
                return pd.read_excel(path, engine='xlrd')
            except Exception as e:
                warnings.warn(f"xlrd è¯»å–å¤±è´¥: {e}; å°è¯•ï¿½?HTML/æ–‡æœ¬æ–¹å¼å›é€€è¯»å–")
                # å›é€€å°è¯•ï¼šæœ‰ï¿½?.xls ï¿½?HTML è¡¨æ ¼ï¼ˆCNKI å¸¸è§ï¿½?
                try:
                    return pd.read_html(path, encoding='gbk')[0]
                except Exception:
                    try:
                        return pd.read_html(path, encoding='utf-8')[0]
                    except Exception:
                        raise

        # å…¶ä»–æ‰©å±•åï¼Œå°è¯• openpyxl å†å›é€€ï¿½?read_html
        try:
            return pd.read_excel(path, engine='openpyxl')
        except Exception:
            try:
                return pd.read_html(path, encoding='gbk')[0]
            except Exception:
                return pd.read_html(path, encoding='utf-8')[0]

    except Exception:
        # æœ€åæŠ›å‡ºå¼‚å¸¸ç»™ä¸Šå±‚å¤„ç†
        raise

# è¯»å– clean_config.yaml
with open(CONFIG_PATH, "r", encoding="utf-8") as f:
    config = yaml.safe_load(f)

# è¯»å–å­—æ®µæ˜ å°„
field_mapping = config.get("field_mapping", {})

def apply_field_mapping(df, field_mapping):
    """
    æ ¹æ® clean_config.yaml ä¸­çš„ field_mapping
    ï¿½?CSV ä¸­çš„åˆ—åç»Ÿä¸€æ˜ å°„ä¸ºæ ‡å‡†å­—æ®µå
    """
    rename_dict = {}
    # è®°å½•åŸå§‹åˆ—åå¿«ç…§ï¼Œåé¢ç”¨äºæç¤ºæœªæ˜ å°„ç›®æ ‡çš„å¯èƒ½å€™ï¿½?
    original_columns = list(df.columns)

    # è§„èŒƒåŒ–å€™é€‰åç§°ï¼ˆå°å†™ã€å»ä¸¤ç«¯ç©ºæ ¼ï¿½?
    norm_map = {}
    for std_field, candidates in field_mapping.items():
        cand_list = candidates if isinstance(candidates, (list, tuple)) else [candidates]
        norm_map[std_field] = [str(c).strip().lower() for c in cand_list if c is not None]

    # å¯¹æ¯ä¸ªåŸå§‹åˆ—åè¿›è¡ŒåŒ¹é…ï¼ˆä¼˜å…ˆç²¾ç¡®åŒ¹é…ï¼Œå†åŒ…å«åŒ¹é…ï¿½?
    for col in df.columns:
        norm_col = str(col).strip().lower()
        matched = False
        # ç²¾ç¡®åŒ¹é…
        for std_field, norm_cands in norm_map.items():
            if norm_col in norm_cands:
                rename_dict[col] = std_field
                matched = True
                break
        # ä¸¥æ ¼æ¨¡å¼ï¼šåªä½¿ç”¨è§„èŒƒåŒ–åçš„ç²¾ç¡®åŒ¹é…ï¼ˆä¸åšåŒ…å«/å­ä¸²åŒ¹é…ï¿½?
        if matched:
            continue

    # åº”ç”¨æ˜ å°„ä½†ä¸æ‰“å°ä¸­é—´æ˜ å°„ç»†èŠ‚ï¼ˆä»…åœ¨æœ«å°¾æ‰“å°æœ€ç»ˆä¿ç•™ç›®æ ‡åˆ—ï¿½?
    # æ„å»ºåå‘æ˜ å°„ï¼šç›®æ ‡å­—ï¿½?-> æºåˆ—åˆ—è¡¨ï¼Œç”¨äºæœ€åè¾“å‡ºè¯´ï¿½?"æºåˆ— -> ç›®æ ‡ï¿½?
    reverse_map = {}
    for orig_col, std_field in rename_dict.items():
        reverse_map.setdefault(std_field, []).append(orig_col)

    if rename_dict:
        df = df.rename(columns=rename_dict)

    # ï¼ˆè°ƒè¯•ï¼‰æ­¤å¤„ä¸å†æ‰“å°ä¸­é—´åˆ—ï¼Œä¸‹é¢å°†åªæ‰“å°æœ€ç»ˆä¿ç•™çš„æ˜ å°„ç›®æ ‡ï¿½?

    # åˆå¹¶å¯èƒ½äº§ç”Ÿçš„åŒååˆ—ï¼ˆä¾‹å¦‚å¤šä¸ªæºåˆ—éƒ½è¢«æ˜ å°„ä¸ºåŒä¸€ç›®æ ‡å­—æ®µï¿½?
    import pandas as _pd
    final_cols = []
    for std_field in field_mapping.keys():
        # æ‰¾å‡ºæ‰€æœ‰åŒååˆ—ï¼ˆä¸¥æ ¼åŒ¹é…ï¼‰
        same_cols = [c for c in df.columns if str(c) == std_field]
        if len(same_cols) > 1:
            # ä½¿ç”¨ä»å·¦åˆ°å³çš„ç¬¬ä¸€ä¸ªéç©ºå€¼ä½œä¸ºåˆå¹¶ç»“ï¿½?
            tmp = df[same_cols].bfill(axis=1).iloc[:, 0].infer_objects(copy=False)
            # åˆ é™¤åŸåˆ—
            df.drop(columns=same_cols, inplace=True)
            # æ’å…¥åˆå¹¶åçš„ï¿½?
            df[std_field] = tmp
        final_cols.append(std_field)

    # ç¡®ä¿åˆ—åå”¯ä¸€
    df = df.loc[:, ~df.columns.duplicated()]

    # åªä¿ç•™æ˜ å°„ä¸­æŒ‡å®šçš„ç›®æ ‡åˆ—ï¼ˆé¿å…å†™å…¥ä¸éœ€è¦çš„æºåˆ—ï¿½?
    mapped_targets = [str(k) for k in field_mapping.keys()]
    keep_cols = [c for c in mapped_targets if c in df.columns]
    if keep_cols:
        df = df[keep_cols]

    # è‹¥æŸäº›é‡è¦ç›®æ ‡ï¼ˆä¾‹å¦‚ keywordsï¼‰æœªæ˜ å°„ï¼Œæç¤ºå¯èƒ½çš„æºåˆ—å€™é€‰
    # æ‰“å°æœ€ç»ˆè¢«è¯†åˆ«å¹¶ä¿ç•™çš„ç›®æ ‡åˆ—ä»¥åŠå®ƒä»¬å¯¹åº”çš„æºåˆ—
    print("ğŸ“‹ æœ€ç»ˆæ˜ å°„å¹¶ä¿ç•™çš„ç›®æ ‡åˆ—:")
    if keep_cols:
        for tgt in keep_cols:
            srcs = reverse_map.get(tgt, [])
            if srcs:
                print(f"   {', '.join(srcs)} -> {tgt}")
            else:
                print(f"   {tgt}")
    else:
        print("   ï¼ˆæ— åŒ¹é…åˆ°çš„ç›®æ ‡åˆ—ï¼‰")

    # å§‹ç»ˆåˆ—å‡ºåŒ…å« 'keyword' æˆ– 'å…³é”®è¯' çš„åŸå§‹åˆ—ï¼Œå¹¶æ ‡æ³¨æ˜¯å¦å·²è¢«æ˜ å°„åˆ° keywords
    candidates = [c for c in original_columns if 'keyword' in str(c).lower() or 'å…³é”®è¯' in str(c)]
    if candidates:
        print('\nå…³é”®è¯å€™é€‰æºåˆ—ï¼ˆåŒ…å« "keyword" æˆ– "å…³é”®è¯"ï¼‰ï¼š')
        for c in candidates:
            mapped_flag = ''
            if 'keywords' in reverse_map and c in reverse_map.get('keywords', []):
                mapped_flag = ' (å·²æ˜ å°„ -> keywords)'
            print(f'   {c}{mapped_flag}')
    else:
        print('\nå…³é”®è¯å€™é€‰æºåˆ—ï¼šæ— ')

    return df

# 1. è¯»å– CSV
# 1. è¯»å–æ•°æ®ï¼šä¼˜å…ˆä» config ä¸­æŒ‡å®šçš„ Excel ç›®å½•è¯»å–æ‰€æœ‰ Excel æ–‡ä»¶å¹¶åˆå¹¶ï¼Œæ‰¾ä¸åˆ°æ—¶å›é€€åˆ° CSV
excel_dir = config.get('data_source', {}).get('excel_dir')
df = None
if excel_dir:
    excel_path = Path(excel_dir)
    if excel_path.exists() and excel_path.is_dir():
        print(f"æ£€æµ‹åˆ° Excel ç›®å½•ï¼š{excel_path}ï¼Œå¼€å§‹è¯»å–æ‰€æœ‰ Excel æ–‡ä»¶...")
        files = sorted([p for p in excel_path.glob('*.xls*')])
        dfs = []
        for f in files:
            try:
                tmp = read_excel_file(f)
                print(f"  è¯»å–: {f.name} -> {len(tmp)} è¡Œ")
                dfs.append(tmp)
            except Exception as e:
                print(f"  è¯»å–å¤±è´¥: {f} - {e}")

        if dfs:
            df = pd.concat(dfs, ignore_index=True)
            print(f"å·²åˆå¹¶ {len(dfs)} ä¸ªæ–‡ä»¶ï¼Œå…± {len(df)} æ¡è®°å½•")

if df is None:
    # å›é€€ CSVï¼šä¼˜å…ˆä½¿ç”¨å½“å‰å·¥ç¨‹ç›®å½•ä¸‹çš„æ•°æ®æ–‡ä»¶ï¼Œé¿å…è¯¯è¯»åˆ°å…¶ä»–ç›®å½•çš„åŒå CSV
    csv_candidates = [
        BASE_DIR / "data" / "all_data.csv",
        BASE_DIR / "data" / "alldata_cleaned.csv",
        BASE_DIR / "data" / "cleaned_data.csv",
        BASE_DIR / "data" / "targetdata_cleaned.csv",
        BASE_DIR / "cleaned_data.csv",
    ]
    csv_default = None
    for p in csv_candidates:
        if p.exists():
            csv_default = str(p)
            break
    if csv_default is None:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°å›é€€ CSVï¼Œå·²å°è¯•: {[str(p) for p in csv_candidates]}")
    try:
        # å…¼å®¹ä¸åŒæ¥æº CSV çš„å¸¸è§ç¼–ç ï¼šä¼˜å…ˆ UTF-8ï¼Œå…¶æ¬¡å›é€€åˆ° GB ç³»ï¼ˆWindows ä¸­æ–‡ç¯å¢ƒå¸¸è§ï¼‰
        last_err = None
        for enc in ("utf-8-sig", "utf-8", "gb18030", "gbk", "cp936"):
            try:
                df = pd.read_csv(csv_default, encoding=enc)
                print(f"å›é€€è¯»å– CSVï¼š{csv_default}ï¼ˆencoding={enc}ï¼‰-> {len(df)} è¡Œ")
                break
            except UnicodeDecodeError as e:
                last_err = e
        if df is None:
            raise last_err or RuntimeError("è¯»å– CSV å¤±è´¥ï¼šæœªçŸ¥ç¼–ç é”™è¯¯")
    except Exception as e:
        print(f"è¯»å– CSV å¤±è´¥ï¼š{e}")
        raise

# åœ¨åº”ç”¨å­—æ®µæ˜ å°„å‰ï¼Œæ‰“å°æœªè¢«æ˜ å°„è¯†åˆ«çš„åŸå§‹åˆ—ï¼Œä¾¿äºè¡¥å……é…ç½®
mapped_candidates = set()
for candidates in field_mapping.values():
    for c in (candidates if isinstance(candidates, (list, tuple)) else [candidates]):
        if c is not None:
            mapped_candidates.add(str(c).strip().lower())

unmapped = [c for c in df.columns if str(c).strip().lower() not in mapped_candidates]
# ä¸æ‰“å°æœªæ˜ å°„çš„åˆ—ï¼Œä»¥é¿å…å†—ä½™è¾“å‡ºï¼ˆå¦‚æœéœ€è¦å¯ï¿½?config ä¸­æ‰“å¼€ï¿½?

# åº”ç”¨å­—æ®µæ˜ å°„ï¼ˆåªæ‰“å°æ˜ å°„åçš„åˆ—ï¼‰
df = apply_field_mapping(df, field_mapping)
# 2. æ›¿æ¢ NaN ï¿½?None
df = df.where(pd.notnull(df), None)

with open(CONFIG_PATH, "r", encoding="utf-8") as f:
    config = yaml.safe_load(f)

db_cfg = config["data_source"]["database"]
table_name = db_cfg.get("table", "papers")
# 3. è¿æ¥ SQLite
db_path = db_cfg.get("db_path", db_cfg.get("database", ""))
conn = sqlite3.connect(db_path)
cursor = conn.cursor()

# 4. æŸ¥è¯¢åŸå§‹æ•°æ®ï¿½?
cursor.execute("SELECT COUNT(*) FROM papers")
original_count = cursor.fetchone()[0] # type: ignore



# 5. ä¸€æ¬¡æ€§è·å–å·²ï¿½?DOI å’Œæ ‡ï¿½?
cursor.execute("SELECT doi FROM papers")
existing_dois = set(row[0] for row in cursor.fetchall())
cursor.execute("SELECT title FROM papers")
existing_titles = set(row[0] for row in cursor.fetchall())

def to_sqlite_value(v):
    """ï¿½?pandas/numpy çš„ç¼ºå¤±å€¼ç»Ÿä¸€è½¬æˆ Noneï¼Œé¿ï¿½?nan å†™å…¥ SQLite"""
    if v is None:
        return None
    # NaN / NaT
    try:
        if pd.isna(v):
            return None
    except Exception:
        pass

    # å­—ç¬¦ä¸²ç©ºï¿½?â€œnanï¿½?
    if isinstance(v, str):
        s = v.strip()
        if s == "" or s.lower() == "nan":
            return None
        return s

    # numpy æ ‡é‡ï¿½?python æ ‡é‡
    if isinstance(v, np.generic):
        return v.item()

    return v


def parse_publish_year(v):
    """ï¿½?publish_date è½¬æˆ int å¹´ä»½ï¼›æ— æ³•è§£æåˆ™è¿”å› None"""
    v = to_sqlite_value(v)
    if v is None:
        return None

    if isinstance(v, (int, np.integer)):
        return int(v)

    if isinstance(v, (float, np.floating)):
        if np.isnan(v):
            return None
        iv = int(v)
        return iv if float(iv) == float(v) else None  # åªæ¥ï¿½?2020.0 è¿™ç§

    if isinstance(v, str):
        s = v.strip()
        # çº¯æ•°ï¿½?æµ®ç‚¹å­—ç¬¦ä¸²ï¼š'2020' / '2020.0'
        try:
            f = float(s)
            iv = int(f)
            if float(iv) == f:
                return iv
        except Exception:
            pass

        # å…œåº•ï¼šä» '2020-01-01' é‡ŒæŠ“ 4 ä½å¹´ï¿½?
        import re
        m = re.search(r"(19\d{2}|20\d{2})", s)
        if m:
            return int(m.group(0))

    return None
# 6. æ’å…¥æ•°æ®ï¿½?
#    - ï¿½?DOI æ—¶æŒ‰ DOI å»é‡
#    - DOI ä¸ºç©ºæ—¶æŒ‰æ ‡é¢˜å»é‡
inserted_count = 0
skip_no_year = 0
skip_dup_doi = 0
skip_dup_title = 0
total_rows = len(df)
print_interval = max(1, total_rows // 10)  # ï¿½?0%æ‰“å°ä¸€æ¬¡è¿›ï¿½?
for idx, row in df.iterrows():
    if inserted_count > 0 and inserted_count % print_interval == 0:
        print(f"è¿›åº¦: {inserted_count}/{total_rows} ({100*inserted_count//total_rows}%)")
    doi = to_sqlite_value(row.get('doi'))
    title = to_sqlite_value(row.get('title'))

    publish_year = parse_publish_year(row.get('publish_date'))
    # è·³è¿‡ publish_date æ— æ³•è§£æçš„è¡Œ
    if publish_year is None:
        skip_no_year += 1
        continue

    # æƒ…å†µ 1ï¼šæœ‰ DOIï¼Œç”¨ DOI å»é‡
    if doi not in (None, ""):
        if doi in existing_dois:
            skip_dup_doi += 1
            continue
    # æƒ…å†µ 2ï¼šæ²¡ï¿½?DOIï¼Œç”¨æ ‡é¢˜å»é‡ï¼ˆæ ‡é¢˜ä¹Ÿä¸ºç©ºåˆ™ä¸åšå»é‡ï¼‰
    else:
        if title in existing_titles:
            skip_dup_title += 1
            continue

    cursor.execute("""
        INSERT INTO papers (doi, journal, keywords, publish_date, target, citations, title, abstract, category)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
    """, (
        doi,
        to_sqlite_value(row.get('journal')),
        to_sqlite_value(row.get('keywords')),
        publish_year,
        to_sqlite_value(row.get('target')),
        to_sqlite_value(row.get('citations')),
        title,
        to_sqlite_value(row.get('abstract')),
        to_sqlite_value(row.get('category'))
    ))

    # ç»´æŠ¤å»é‡é›†åˆ
    if doi not in (None, ""):
        existing_dois.add(doi)
    if title not in (None, ""):
        existing_titles.add(title)

    inserted_count += 1

conn.commit()

# 7. æŸ¥è¯¢æ’å…¥åçš„æ•°æ®ï¿½?
cursor.execute("SELECT COUNT(*) FROM papers")
final_count = cursor.fetchone()[0] # type: ignore

cursor.close()
conn.close()

print(f"\nåŸæœ‰æ•°æ®é‡ï¼š{original_count}")
print(f"æœ¬æ¬¡æ’å…¥æ–°æ•°æ®é‡ï¼š{inserted_count}")
print(f"æ’å…¥åæ€»æ•°æ®é‡ï¼š{final_count}")

# 8. è·³è¿‡åŸå› ç»Ÿè®¡ï¼ˆä¾¿äºè§£é‡Šä¸ºä»€ä¹ˆè¯»åˆ° 3 ä¸‡æ¡ä½†åªæ’å…¥ 1 ä¸‡å¤šï¼‰
total_rows = len(df)
skipped_total = total_rows - inserted_count
print("\nğŸ“Š è·³è¿‡åŸå› ç»Ÿè®¡ï¼š")
print(f"æ€»è¡Œæ•°: {total_rows}")
print(f"æˆåŠŸæ’å…¥: {inserted_count}")
print(f"æ€»è·³è¿‡: {skipped_total}")
print(f"  - publish_date æ— æ³•è§£æ: {skip_no_year}")
print(f"  - DOI é‡å¤è·³è¿‡: {skip_dup_doi}")
print(f"  - æ ‡é¢˜é‡å¤è·³è¿‡: {skip_dup_title}")

